{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with open('lorem_ipsum.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    text = text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpe tokenizer\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids[:-1], ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and (ids[i], ids[i + 1]) == pair:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "def byte_pair_encoding(text, num_merges):\n",
    "    tokens = text.encode('utf-8', errors='replace')\n",
    "    tokens_processed = list(map(int, tokens))\n",
    "    merges = {}\n",
    "    for i in range(num_merges):\n",
    "        stats = get_stats(tokens_processed)\n",
    "        max_pair = max(stats, key=stats.get)\n",
    "        tokens_processed = merge(tokens_processed, max_pair, 256 + i)\n",
    "        merges[max_pair] = 256 + i\n",
    "        print(f'merging pair {max_pair} into {256 + i}')\n",
    "    # Save the merges to a file\n",
    "    with open('merges.txt', 'w') as f:\n",
    "        for pair, token_id in merges.items():\n",
    "            f.write('\\t'.join([str(pair[0]), str(pair[1]), str(token_id)]) + '\\n')\n",
    "    return tokens_processed, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 280\n",
    "num_of_merges = vocab_size - 256\n",
    "tokens_processed, merges = byte_pair_encoding(text, num_of_merges)\n",
    "\n",
    "vocab = {idx : bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "# print tokens with largest idx\n",
    "def decode(ids):\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    new_text = tokens.decode('utf-8', errors='replace')\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    tokens = list(text.encode('utf-8'))    \n",
    "    if len(tokens) >= 2:\n",
    "        for pair, idx in merges.items():\n",
    "            if len(tokens) < 2:\n",
    "                break\n",
    "            tokens = merge(tokens, pair, idx)   \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is bpe tokenizer test\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode('This is bpe tokenizer test')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
